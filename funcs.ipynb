{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO IMPORT a JUPYTER NOTEBOOK inside a code:::::::::::::::::::::::::::::\n",
    "\n",
    "# https://stackoverflow.com/questions/44116194/import-a-function-from-another-ipynb-file\n",
    "\n",
    "# !pip install ipynb\n",
    "# !pip3 install ipynb\n",
    "\n",
    "# import ipynb.fs.full.funcs as om\n",
    "\n",
    "# testing = om.func1(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(num):\n",
    "    print(2*num)\n",
    "    return num*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO IMPORT a FTP PYTHON file inside the code:::::::::::::::::::::::::::::\n",
    "\n",
    "def copyInnerCodeToImportFuncsFromFTP():\n",
    "    # import a function library from FTP to the project\n",
    "    def importFromFTP(csvhost, csvuser, csvpwd, filename, subfolder='', act=''):\n",
    "        import ftplib\n",
    "        import pandas as pd\n",
    "        ftp = ftplib.FTP(host=list(pd.read_csv(csvhost))[0], user=list(pd.read_csv(csvuser))[0],\n",
    "                         passwd=list(pd.read_csv(csvpwd))[0], acct=act)\n",
    "        if subfolder != '':\n",
    "            ftp.cwd(\"/\" + subfolder)\n",
    "        gFile = open(filename, \"wb\")  # wb= write mode and binary mode\n",
    "        ftp.retrbinary('RETR ' + filename, gFile.write)\n",
    "        gFile.close()\n",
    "        ftp.quit()\n",
    "\n",
    "    # Download the python file to project folder\n",
    "    importFromFTP('c:/c/host.csv', 'c:/c/un.csv', 'c:/c/pwd.csv', 'ftpfuncs.py', '', '')\n",
    "    # import the python file to project\n",
    "    import ftpfuncs as omftp\n",
    "    # Test a function\n",
    "    # omftp.print_('test using remote ftp function.')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "################    convert below extrnal ftp func block to single func#############\n",
    "# # https://www.informit.com/articles/article.aspx?p=686162&seqNum=7#:~:text=The%20ftplib%20module%20included%20in,passwd%5D%5D%5D)%20method.\n",
    "# def getFTPFile(hst, un, pwd, filename, subfolder='', act=''):\n",
    "#     import ftplib\n",
    "#     # Open ftp connection\n",
    "#     ftp = ftplib.FTP(host=hst, user=un, passwd=pwd, acct=act)\n",
    "#     # ftp = ftplib.FTP('hst, 'anonymous', 'bwdayley@novell.com')\n",
    "#     # ftp = ftplib.FTP('ftp.novell.com', 'anonymous', 'bwdayley@novell.com')\n",
    "#     # List the files in the current directory\n",
    "#     print(\"ROOT File List:\")\n",
    "#     files = ftp.dir()\n",
    "#     print(files)\n",
    "#     # Get the file\n",
    "#     if subfolder != '':\n",
    "#         ftp.cwd(\"/\" + subfolder)\n",
    "#         # ftp.cwd(\"/pub\")\n",
    "#         print(\"SUBFOLDER File List:\")\n",
    "#         files = ftp.dir()\n",
    "#         print(files)\n",
    "#     try:\n",
    "#         gFile = open(filename, \"wb\")  # wb= write mode and binary mode\n",
    "#         # ftp.retrbinary('RETR Readme', gFile.write)\n",
    "#         ftp.retrbinary('RETR ' + filename, gFile.write)\n",
    "#     except Exception as err:\n",
    "#         print('ftplib.error_perm: 550 {}: No such file or directory'.format(filename))\n",
    "#     gFile.close()\n",
    "#     ftp.quit()\n",
    "#     try:\n",
    "#         # Print the file contents\n",
    "#         print(\"\\nFile Output:\")\n",
    "#         gFile = open(filename, \"r\")\n",
    "#         buff = gFile.read()\n",
    "#         print(buff)\n",
    "#     except Exception as err:\n",
    "#         print('No file downloaded!!!')\n",
    "#     gFile.close()\n",
    "\n",
    "# def readCSV1stVal(filepath):\n",
    "#     df = pd.read_csv(filepath)  # rets a list if csv is single val\n",
    "#     # return df.to_string()  # Empty DataFrame Columns: [194.59.164.38] Index: []\n",
    "#     return list(df)[0]  # 194.59.164.38\n",
    "#     # return dfItem1(df, 0, 0)  # index 0 is out of bounds for axis 0 with size 0\n",
    "\n",
    "# EXAMPLE:::::::::::::::::::::::\n",
    "# print(readCSV1stVal('c:/c/host.csv')) # 194.59.164.38\n",
    "\n",
    "# hhsstt = readCSV1stVal('c:/c/host.csv')\n",
    "# uunn = readCSV1stVal('c:/c/un.csv')\n",
    "# ppwwdd = readCSV1stVal('c:/c/pwd.csv')\n",
    "# getFTPFile(hhsstt, uunn, ppwwdd, 'myfuncs.py')\n",
    "# import myfuncs as omftp\n",
    "# omftp.printt_('omiiiiid myfuncs test')\n",
    "\n",
    "################    convert above extrnal ftp func block to single func#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## to see the list of installed packages::::::::::\n",
    "# pip list\n",
    "\n",
    "\n",
    "# # To install libs use one of these:\n",
    "# !conda install Scikit-learn --yes\n",
    "# !pip3 install Scikit-learn \n",
    "# !pip install Scikit-learn \n",
    "\n",
    "# # To see doc about each function\n",
    "# help(Prophet) \n",
    "# help(Prophet.fit)\n",
    "# plot_plotly??\n",
    "\n",
    "\n",
    "########## To Open jupyter notebooks in pycharm::::::::::::::::::::::::::::::::\n",
    "# instal jupyter package first\n",
    "# pip install jupyter\n",
    "\n",
    "\n",
    "\n",
    "######## to use kite code auto complete\n",
    "# !pip3 install jupyter-kite\n",
    "\n",
    "\n",
    "\n",
    "###### to UPGRADE the pip installer\n",
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29\n",
    "\n",
    "# # To chnage jupyterto dark mode: rem reload after each change\n",
    "# !pip install jupyterthemes\n",
    "# import jupyterthemes as jt\n",
    "# !jt -l \n",
    "# Available Themes: DARK: chesterish, grade3, monokai, oceans16, onedork, solarizedd,,, LIHJT: gruvboxd, gruvboxl, solarizedl\n",
    "# !jt -t solarizedd \n",
    "# selecting a particular theme\n",
    "# !jt -r \n",
    "# reverting to original Theme\n",
    "\n",
    "# # Notebook Extensions (nbextensions)\n",
    "# !conda install -c conda-forge jupyter_nbextensions_configurator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline \n",
    "# in jupyter notebook palces the charts inside the code\n",
    "\n",
    "\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as prnd  # python random\n",
    "\n",
    "from IPython.display import IFrame  # to show html file in code\n",
    "\n",
    "\n",
    "# # To have conda instead of pip\n",
    "# !pip3 install conda # ERROR\n",
    "# !pip3 install anaconda\n",
    "# OR\n",
    "# download miniconda and install:\n",
    "# https://docs.conda.io/en/latest/miniconda.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# !pip3 install networkx\n",
    "# !pip3 install pandas_profiling\n",
    "# from pandas_profiling import ProfileReport\n",
    "# import pandas_profiling\n",
    "\n",
    "\n",
    "# pip install pandas_datareader\n",
    "\n",
    "\n",
    "\n",
    "# import scipy as sp\n",
    "# from scipy.stats import boxcox\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "\n",
    "\n",
    "# # Lib for Stock market technical analysis $$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "# https://blog.quantinsti.com/install-ta-lib-python/#windows\n",
    "# pip install cython # adding C++ boost to Python\n",
    "# # first install the \"whl\" file: https://www.lfd.uci.edu/~gohlke/pythonlibs/#ta-lib\n",
    "# !pip3 install yfinance \n",
    "# import yfinance as yf\n",
    "# import talib as ta\n",
    "# # but first install this: \n",
    "# # https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "\n",
    "\n",
    "\n",
    "# import plotly as py\n",
    "# import dash # !pip install dash\n",
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn import datasets, linear_model\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import sweetviz as sv  # auto EDA plots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import (\n",
    "#     r2_score,\n",
    "#     mean_squared_error,\n",
    "#     mean_absolute_error,\n",
    "#     make_scorer)\n",
    "\n",
    "\n",
    "\n",
    "# ########## to read EXCEL files:::::::::::::::\n",
    "# pip install xlrd\n",
    "# from pandas import ExcelFile\n",
    "# df = pd.read_excel('../../DATA/Iris.xls', sheet_name = 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryExceptFullSampleCode():\n",
    "    ######### # TRY-EXCEPT ERROR HANDLIONG FULL SAMPLE:::::::::::::::::\n",
    "    # https://www.w3schools.com/python/python_try_except.asp\n",
    "    try:  # try main code execution\n",
    "        print(x)  # Variable x is not defined (NameError)\n",
    "        # raise conditional ERR\n",
    "        x = -1\n",
    "        if x < 0:\n",
    "            raise Exception(\"Sorry, no numbers below zero\")  # ANY Exception\n",
    "        x = \"hello\"\n",
    "        if not type(x) is int:\n",
    "            raise TypeError(\"Only integers are allowed\")  # TypeError Exception\n",
    "    except NameError:  # catch specific err\n",
    "        print(\"Variable x is not defined\")\n",
    "    except Exception as err:  # catch any errs\n",
    "        print(myfuncname.__name__ + ' has this error: ' + str(err.args))\n",
    "        # ex. getFile has this error: (2, 'File data/michelson_speed_of_light.csv does not exist')\n",
    "    else:  # main code executed with NO ERR\n",
    "        print(\"Nothing went wrong\")\n",
    "    finally:  # runs anyway: to close objects and clean up resources\n",
    "        print(\"The 'try except' is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import enum\n",
    "from enum import Enum\n",
    "\n",
    "# https://stackoverflow.com/questions/36932/how-can-i-represent-an-enum-in-python\n",
    "class enmRandTyp_(Enum):\n",
    "    random, normal, binominal, poisson, exponential = range(5)\n",
    "    # very rare events: POSSION for discrete EXPONENTIAL for continues\n",
    "    # non rare events: BINOMINAL for discrete NORMAL for continues\n",
    "\n",
    "class enmDTyp_(Enum):\n",
    "    scalar, tuple, list, set, dict, dataframe = range(6)\n",
    "\n",
    "class enmDTyp2_(Enum):\n",
    "    all, obj, num = range(3)\n",
    "\n",
    "class enmCorr_(Enum):\n",
    "    pearson, kendall, spearman = range(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "__sufx__ = ':::'\n",
    "__prfx__ = '::::::::::'\n",
    "\n",
    "\n",
    "def print_(*args):\n",
    "    # ex. print_(__prfx__, 'TYPE', __sufx__, type(realdata))\n",
    "    # result: ::::::::::TYPE:::<type 'list'>\n",
    "    # not using result: ('::::::::::', 'TYPE', ':::', <type 'list'>)\n",
    "    out = ''\n",
    "    for itm in args:\n",
    "        out += str(itm)\n",
    "    print(out)\n",
    "\n",
    "\n",
    "def prfx(msg):\n",
    "    return str(__prfx__) + str(msg)\n",
    "\n",
    "\n",
    "def sufx(msg):\n",
    "    return str(msg) + str(__sufx__)\n",
    "\n",
    "\n",
    "def prfxsufx(msg):\n",
    "    return str(__prfx__) + str(msg) + str(__sufx__)\n",
    "\n",
    "\n",
    "def prfxsufxsufx(msg1, msg2):\n",
    "    return str(__prfx__) + str(msg1) + str(__sufx__) + str(msg2) + str(__sufx__)\n",
    "\n",
    "\n",
    "def errInfo(errObj):\n",
    "    return str(errObj.args)\n",
    "\n",
    "\n",
    "def funcName(func):\n",
    "    out = ''\n",
    "    try:\n",
    "        out = func.__name__\n",
    "    except:\n",
    "        try:\n",
    "            out = str(func)\n",
    "        except Exception as err:\n",
    "            out = ''\n",
    "            printError(err, funcName)\n",
    "    return out\n",
    "\n",
    "\n",
    "def printError(errObj, caller=''):\n",
    "    func = funcName(printError)\n",
    "    if caller != '': func = funcName(caller)\n",
    "    print_(prfxsufxsufx('!!!!ERORR!!!!', errInfo(errObj)), __sufx__, 'CALLER::', func)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "# MAPE symmetric_mean_absolute_percentage_error\n",
    "def regMAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + 1.))) * 100        \n",
    "# SMAPE symmetric_mean_absolute_percentage_error\n",
    "def regSMAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs(2 * (y_true - y_pred) / (y_true + y_pred))) * 100\n",
    "# PE percentage_error\n",
    "def regPE(y_true, y_pred):\n",
    "    return ((y_true - y_pred) / y_true) * 100\n",
    "# AE absolute_error\n",
    "def regAE(y_true, y_pred):\n",
    "    return np.abs(y_true - y_pred)\n",
    "# MAE mean_absolute_error\n",
    "def regMAE(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "# RMSE root_mean_square_error\n",
    "def regRMSE(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "# RMSPE root_mean_square_percent_error\n",
    "def regRMSPE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))\n",
    "# R2 R^2 r_squared\n",
    "def regR2(y_true, y_pred):\n",
    "    from sklearn.metrics import r2_score\n",
    "    return r2_score(y_true, y_pred)\n",
    "# print_error\n",
    "def regPrintErr(y_true, y_pred, printerrs=True, decimaldigits=2):\n",
    "    import numpy as np\n",
    "    mape = round(regMAPE(y_true, y_pred), decimaldigits)\n",
    "    smape = round(regSMAPE(y_true, y_pred), decimaldigits)\n",
    "    mae = round(regMAE(y_true, y_pred), decimaldigits)\n",
    "    rmse = round(regRMSE(y_true, y_pred), decimaldigits)\n",
    "    std = round(np.std(y_true), decimaldigits)\n",
    "    rmsestd = round(rmse/std,decimaldigits)\n",
    "    r2 = round(regR2(y_true, y_pred), decimaldigits)\n",
    "    if printerrs:\n",
    "        print('MAPE     : {}'.format(mape))\n",
    "        print('MAE      : {}'.format(mae))\n",
    "        print('RMSE     : {}'.format(rmse))\n",
    "        print('RMSE/STD : {}'.format(rmsestd))\n",
    "        print('Data STD : {}'.format(std))\n",
    "        print('R^2     ^: {}'.format(r2))\n",
    "    return mape, mae, rmse, rmsestd, std, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwargsget(kwargs, argname):\n",
    "    return kwargs.get(argname)\n",
    "\n",
    "\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "def fileExists(filname):\n",
    "    return path.exists(filname)\n",
    "\n",
    "\n",
    "# to show html file in code\n",
    "from IPython.display import IFrame\n",
    "def showHTML(file, w='100%', h='800'):\n",
    "    return IFrame(file, width=w, height=h) # width='200%', height='1500'\n",
    "\n",
    "\n",
    "\n",
    "def getFile(filename, optColNamesList=[], removeHeadRow=True, searchColName='', searchVal=''):\n",
    "    \"\"\"reads a file\"\"\"\n",
    "    \"\"\"able to read only specific columns if column list is provided\"\"\"\n",
    "    \"\"\"able to read only specific rows if search column and value is provided\"\"\"\n",
    "\n",
    "    # https://www.w3schools.com/python/python_try_except.asp\n",
    "    try:  # main code\n",
    "        # Initialize reader object: urb_pop_reader\n",
    "        filereader = pd.read_csv(filename, chunksize=1000)\n",
    "        # Initialize empty DataFrame: data\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        # Iterate over each DataFrame chunk\n",
    "        for df_chunk in filereader:\n",
    "            # Check out specific country: df_pop_ceb\n",
    "\n",
    "            df_filtered = df_chunk  # whole data\n",
    "            # if np.array(optColNamesList).size == 0:\n",
    "            # df_filtered = df_chunk  # whole data\n",
    "            # else:\n",
    "            # df_filtered = df_chunk[optColNamesList]  # some cols\n",
    "            # # https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/index.html\n",
    "\n",
    "            if searchColName != '':\n",
    "                # df_filtered = df_chunk[df_chunk[searchColName] == searchVal]\n",
    "                df_filtered = df_filtered[df_filtered[searchColName] == searchVal]\n",
    "\n",
    "            # Append DataFrame chunk to data: data\n",
    "            # data = data.append(df_filtered)\n",
    "            if np.array(optColNamesList).size == 0:\n",
    "                data = data.append(df_filtered)\n",
    "            else:\n",
    "                data = data.append(df_filtered[optColNamesList])\n",
    "            # # https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/index.html\n",
    "\n",
    "        # outlist = list(data)  # ERR only header row\n",
    "\n",
    "        outlist = []\n",
    "        # https://datatofish.com/convert-pandas-dataframe-to-list/\n",
    "        if not removeHeadRow:\n",
    "            outlist = [data.columns.values.tolist()] + data.values.tolist()  # OK vals plus header row\n",
    "            # outlist = [data.columns.values.tolist()] + data.to_numpy()  # OK vals plus header row\n",
    "        if removeHeadRow:\n",
    "            outlist = data.values.tolist()  # OK vals with no header row\n",
    "            # outlist = data.to_numpy()  # OK vals with no header row\n",
    "\n",
    "        if np.array(optColNamesList).size == 1:\n",
    "            return list(np.array(outlist).flatten())\n",
    "        else:\n",
    "            return list(outlist)\n",
    "    except Exception as err:  # other errs\n",
    "        printError(err, getFile)\n",
    "        # :::getFile::::::::::(2, 'File data/michelson_speed_of_light.csv does not exist')\n",
    "\n",
    "\n",
    "# EXAMPLE:::::::::::::::::::::::::::::::::::::::::\n",
    "# realdata = range(0, 100)  # fix this\n",
    "# realdata = getFile('data/michelson_speed_of_light.csv', ['velocity of light in air (km/s)'], True)\n",
    "# getEDA(realdata)  # <type 'list'>\n",
    "\n",
    "\n",
    "# realdata = np.array(realdata)  # WAR: transpose not working on 1D arr\n",
    "# getEDA(realdata)  # <type 'numpy.ndarray'>\n",
    "# realdata = np.array(realdata)[np.newaxis]  # SOL 1st convert to 2D then np.transpose\n",
    "# realdata = np.array([realdata]) # SOL 1st convert to 2D then np.transpose\n",
    "# realdata = np.transpose(realdata)\n",
    "# tst = trnsps(realdata) # ERR\n",
    "#\n",
    "# realdataecdfx, realdataecdfy = f.getECDF(realdata)\n",
    "#\n",
    "# realdatamean = np.mean(realdata)\n",
    "# realdatastd = np.std(realdata)\n",
    "# optimalmean = realdatamean  # fix this with bootstrap replica\n",
    "# optimalstd = realdatastd  # fix this with bootstrap replica\n",
    "#\n",
    "# theorydata = f.getRandomData(f.enmRandTyp_.normal, 10000, 42, optimalmean, optimalstd)\n",
    "# theorydataecdfx, theorydataecdfy = f.getECDF(theorydata)\n",
    "# theorydatahalf = f.getRandomData(f.enmRandTyp_.normal, 10000, 42, optimalmean / 2, optimalstd)\n",
    "# theorydataecdfxhalf, _ = f.getECDF(theorydatahalf)\n",
    "# theorydatadouble = f.getRandomData(f.enmRandTyp_.normal, 10000, 42, optimalmean * 2, optimalstd)\n",
    "# theorydataecdfxdouble, _ = f.getECDF(theorydatadouble)\n",
    "#\n",
    "# f.plotHisto(realdata, 'real vs normal', 'samples', 'PDF', 20,  theorydata, theorydatahalf, theorydatadouble)\n",
    "# f.plotLine(realdataecdfx, realdataecdfy, 'real vs normal', 'samples', 'CDF', [theorydataecdfx, theorydataecdfy],\n",
    "#            [theorydataecdfxhalf, theorydataecdfy], [theorydataecdfxdouble, theorydataecdfy])\n",
    "\n",
    "\n",
    "# tst = getFile('nyc.csv', ['co', 'pm25'])\n",
    "# tst2 = getFile('nyc.csv', ['co', 'ozone'])\n",
    "# plotLine2D(tst, 'pm25')\n",
    "# plotLine2D(tst2, 'pm25 n ozone')\n",
    "\n",
    "# tst = getFile('nyc.csv', ['co'])\n",
    "# print('type::::::::', type(tst))\n",
    "# print(head(tst, 10))\n",
    "# tst2 = getFile('nyc.csv', ['pm25'])\n",
    "# tst3 = getFile('nyc.csv', ['ozone'])\n",
    "# plotHisto(tst2)\n",
    "# plotLine(tst, tst2, '', \"\", \"\", [tst, tst3])\n",
    "\n",
    "# tst = getFile('nyc.csv', ['co'])\n",
    "# x, y = getECDF(tst)\n",
    "# print('type::::::::', type(x))\n",
    "# print(head(x, 10))\n",
    "# plotLine(x, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lab 321:::::::::::::::::::::::::::\n",
    "\n",
    "# def bar_charts(df, feature):\n",
    "#     '''\n",
    "#     Inputs:\n",
    "#     df: Dataset\n",
    "#     feature: Name of Feature to Check With Survived\n",
    "#     '''\n",
    "#     _agg = {\n",
    "#         'PassengerId': 'count'\n",
    "#     }\n",
    "#     _groupby = ['Survived', feature]\n",
    "#\n",
    "#     df_feature = df.groupby(by=_groupby).agg(_agg)\n",
    "#\n",
    "#     ax = df_feature.unstack().plot(kind='bar', figsize=(15, 6))\n",
    "#     plt.legend(list(df_feature.index.levels[1].unique()))\n",
    "#     plt.xlabel('Survived')\n",
    "#     plt.xticks(np.arange(2), ('No', 'Yes'))\n",
    "#     plt.show();\n",
    "\n",
    "\n",
    "# def create_age_class(x):\n",
    "#     if x > 60:\n",
    "#         age_class = 5\n",
    "#     elif x > 35 and x <= 60:\n",
    "#         age_class = 4\n",
    "#     elif x > 25 and x <= 35:\n",
    "#         age_class = 3\n",
    "#     elif x > 16 and x <= 25:\n",
    "#         age_class = 2\n",
    "#     else:\n",
    "#         age_class = 1\n",
    "#     return age_class\n",
    "#\n",
    "# titanic['AgeClass']=titanic['Age'].apply(create_age_class)\n",
    "# titanic.head() # extra AgeClass col added\n",
    "\n",
    "# https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyDataFrame():\n",
    "    tmp = pd.DataFrame()\n",
    "    print('TYPE::::', type(tmp))\n",
    "    print('DIM::::', tmp.ndim)\n",
    "    print('SHAPE::::', tmp.shape)\n",
    "    print('SIZE::::', tmp.size)\n",
    "    print('VALS::::', tmp)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "# def emptyNpArray(rows=1):\n",
    "#     tmp = np.empty(rows)\n",
    "#     print('TYPE::::', type(tmp))\n",
    "#     print('DIM::::', tmp.ndim)\n",
    "#     print('SHAPE::::', tmp.shape)\n",
    "#     print('SIZE::::', tmp.size)\n",
    "#     print('VALS::::', tmp)\n",
    "#     return tmp\n",
    "\n",
    "\n",
    "def emptyNpArray():\n",
    "    return np.array([])\n",
    "\n",
    "\n",
    "def dictEmpty():\n",
    "    return dict()\n",
    "\n",
    "\n",
    "def convertToList(var):\n",
    "    # BEFORE: array([1.21409585e-315, 7.31624424e-315, 1.35999940e-315, 8.83204792e-316, 1.90979621e-313])\n",
    "    # AFTER: [1.21409585e-315, 7.31624424e-315, 1.359999405e-315, 8.8320479e-316, 1.90979621187e-313]\n",
    "    return list(var)\n",
    "\n",
    "\n",
    "def listEmpty():\n",
    "    return []\n",
    "\n",
    "\n",
    "def listAdd(lstname, val):\n",
    "    lstname += [val]\n",
    "\n",
    "\n",
    "def list2D(lstname):\n",
    "    return lstname[:, np.newaxis]\n",
    "    # TypeError: list indices must be integers, not tuple\n",
    "\n",
    "\n",
    "# # EXAMPL:::::::::::::::\n",
    "# arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n",
    "# print(arr)  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n",
    "# print(list2D(arr))  # TypeError: list indices must be integers, not tuple\n",
    "\n",
    "\n",
    "def listGen(*itms):\n",
    "    # out = []\n",
    "    # # if len(itms) > 0:\n",
    "    # for itm in itms:\n",
    "    #     # for i in range(len(itms)):\n",
    "    #     # out += itm  # TypeError: 'int' object is not iterable\n",
    "    #     # out.extend(itm) # TypeError: 'int' object is not iterable\n",
    "    #     # SOL: wrap whole returning int val with []\n",
    "    #     out += [itm]  # OK\n",
    "    #     # out += [itms[i]] # OK\n",
    "    # return out  # <type 'list'> [1, 2, 3, 4, 'tst', 55]\n",
    "    # return np.array(out).flatten() # <type 'numpy.ndarray'> ['1' '2' '3' '4' 'tst' '55']\n",
    "    # return np.array(out).tolist()  # <type 'list'> ['1', '2', '3', '4', 'tst', '55']\n",
    "    out = []\n",
    "    out += itms\n",
    "    return out\n",
    "\n",
    "\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# print(lst)  # [1, 2, 3, 4, 'tst', 55]\n",
    "# print(type(lst))  # <type 'list'>\n",
    "# lstempty = listGen()\n",
    "# print(lstempty)  # []\n",
    "# print(type(lstempty))  # <type 'list'>\n",
    "# lst2 = listGen(lst)\n",
    "# print(lst2)  # WAR: [[1, 2, 3, 4, 'tst', 55]]\n",
    "# print(type(lst2))  # <type 'list'>\n",
    "# listAdd(lst2, 999)\n",
    "# print(lst2)  # [[1, 2, 3, 4, 'tst', 55], 999]\n",
    "# print(type(lst2))  # <type 'list'>\n",
    "# listAdd(lst2, lst)\n",
    "# print(lst2)  # [[1, 2, 3, 4, 'tst', 55], 999, [1, 2, 3, 4, 'tst', 55]]\n",
    "# print(type(lst2))  # <type 'list'>\n",
    "\n",
    "\n",
    "def listItem(lstname, idx):\n",
    "    return lstname[idx]\n",
    "\n",
    "\n",
    "def listRev(listname):\n",
    "    # https://dbader.org/blog/python-reverse-list\n",
    "    # mylist[::-1]\n",
    "    # call mylist.reverse() then return mylist\n",
    "    # list(reversed(mylist))\n",
    "    listname.reverse()\n",
    "    return listname\n",
    "\n",
    "\n",
    "def listWithIdxCol(listname):\n",
    "    \"\"\"USAGE:\n",
    "    for i, itm in listWithIdxCol(lst):\n",
    "    print('myidx: {} myval: {}'.format(i, itm))\n",
    "    \"\"\"\n",
    "    return enumerate(listname)\n",
    "\n",
    "\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# for i, itm in listWithIdxCol(lst): # OK\n",
    "#     print('idx: {} itm: {}'.format(i, itm))\n",
    "\n",
    "def listCopy(listname):\n",
    "    return listname.copy()\n",
    "\n",
    "\n",
    "def listShuffle(listname, sampleSize=0):\n",
    "    if sampleSize <= 0:\n",
    "        sampleSize = len(listname)\n",
    "    return prnd.sample(listname, sampleSize)\n",
    "\n",
    "\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# print('Random suffle: {}'.format(listShuffle(lst)))  # OK: [55, 2, 4, 'tst', 3, 1]\n",
    "# print('Random suffle: {}'.format(listShuffle(lst, 3)))  # OK: [3, 'tst', 1]\n",
    "\n",
    "\n",
    "def listsToZipTuples(list1, list2):\n",
    "    return zip(list1, list2)\n",
    "\n",
    "\n",
    "def listsToDict(list1, list2):\n",
    "    # return dict(list1, list2)  # ERR: 'dict expected at most 1 arguments, got 2'\n",
    "    # return dict({list1, list2}) # ERR: \"unhashable type: 'list'\"\n",
    "    # return dict([list1, list2])  # ERR: 'dictionary update sequence element #0 has length 6; 2 is required'\n",
    "    return dict(zip(list1, list2))  # OK\n",
    "\n",
    "\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# lst2 = listGen('a', 'b', 'c', 'd', 'e', 'f')\n",
    "# ziptups = listsToZipTuples(lst2, lst)\n",
    "# print('tbl type: {}'.format(type(ziptups)))  # OK: <type 'list'>\n",
    "# print('table: {}'.format(ziptups))  # OK: [('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 'tst'), ('f', 55)]\n",
    "# dicttt = listsToDict(lst2, lst)\n",
    "# print('tbl type: {}'.format(type(dicttt)))  # OK: <type 'dict'>\n",
    "# print('table: {}'.format(dicttt))  # OK: {'a': 1, 'c': 3, 'b': 2, 'e': 'tst', 'd': 4, 'f': 55}\n",
    "# dicttt = listsToDict(lst, lst2)\n",
    "# print('table: {}'.format(dicttt))  # OK: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 55: 'f', 'tst': 'e'}\n",
    "\n",
    "\n",
    "def listSort(listname, desc=False):\n",
    "    # You cannot sort a list that contains BOTH string values AND numeric values\n",
    "    # WRONG: [1, 2, 3, 4, 55, 'tst'] moves strings to the end\n",
    "    return sorted(listname, reverse=desc)\n",
    "\n",
    "\n",
    "def dictSortedValsAsKeyList(dictName, desc=False):\n",
    "    return sorted(dictName, key=dictName.get, reverse=desc)\n",
    "\n",
    "\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# print('sorted list:', listSort(lst))  # OK: [1, 2, 3, 4, 55, 'tst']\n",
    "# lst2 = listGen('a', 'b', 'c', 'd', 'e', 'f')\n",
    "# print('sorted list desc:', listSort(lst2, True))  # OK: ['f', 'e', 'd', 'c', 'b', 'a']\n",
    "# tbl = listsToDict(lst2, lst)\n",
    "# print('dict:', tbl)  # OK: {'a': 1, 'c': 3, 'b': 2, 'e': 'tst', 'd': 4, 'f': 55}\n",
    "# print('sorted dict key list:', dictSortedValsAsKeyList(tbl, True))  # OK: ['e', 'f', 'd', 'c', 'b', 'a'])\n",
    "\n",
    "def listSortedWithIdxCol(listname, desc=False):\n",
    "    \"\"\"USAGE:\n",
    "    for i, itm in listSortedWithIdxCol(listname):\n",
    "    print('myidx: {} myval: {}'.format(i, itm))\n",
    "    \"\"\"\n",
    "    return enumerate(listSort(listname, desc))\n",
    "\n",
    "\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# print('unsorted: ', lst)  # OK: [1, 2, 3, 4, 'tst', 55]\n",
    "# print('sorted enum: ', listSortedWithIdxCol(lst))  # OK: <enumerate object at 0x000000000AA47B38>\n",
    "# for i, itm in listSortedWithIdxCol(lst):  # OK: myidx: 0 myval: 1 ... myidx: 5 myval: tst\n",
    "#     print('myidx: {} myval: {}'.format(i, itm))\n",
    "\n",
    "\n",
    "def dictSortedValsAsKeyListWithIdxCol(dictName, desc=False):\n",
    "    \"\"\"USAGE:\n",
    "    for i, itm in listSortedWithIdxCol(listname):\n",
    "    print('myidx: {} myval: {}'.format(i, itm))\n",
    "    \"\"\"\n",
    "    return enumerate(dictSortedValsAsKeyList(dictName, desc))\n",
    "\n",
    "\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# print('lst:', lst)  # OK: [1, 2, 3, 4, 'tst', 55]\n",
    "# lst2 = listGen('a', 'b', 'c', 'd', 'e', 'f')\n",
    "# print('lst2:', lst2)  # OK: ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "# tbl = listsToDict(lst2, lst)\n",
    "# print('dict lst2 lst:', tbl)  # OK: {'a': 1, 'c': 3, 'b': 2, 'e': 'tst', 'd': 4, 'f': 55}\n",
    "# print('desc sorted dict key list:', dictSortedValsAsKeyList(tbl, True))  # OK: ['e', 'f', 'd', 'c', 'b', 'a'])\n",
    "# print('desc sorted enum:', dictSortedValsAsKeyListWithIdxCol(tbl, True))  # OK: <enumerate object at 0x000000000A87BC78>\n",
    "# for i, itm in dictSortedValsAsKeyListWithIdxCol(tbl, True):  # OK: myidx: 0 myval: e ... myidx: 5 myval: a\n",
    "#     print('myidx: {} myval: {}'.format(i, itm))\n",
    "\n",
    "\n",
    "def dictFilter(dictname, searchVal, operator='=='):\n",
    "    out = dict()\n",
    "    for k in dictname.keys():\n",
    "        if operator == '==':\n",
    "            if dictname[k] == searchVal:\n",
    "                out[k] = dictname[k]\n",
    "        if operator == '!=':\n",
    "            if dictname[k] != searchVal:\n",
    "                out[k] = dictname[k]\n",
    "        if operator == '<=':\n",
    "            if dictname[k] <= searchVal:\n",
    "                out[k] = dictname[k]\n",
    "        if operator == '>=':\n",
    "            if dictname[k] >= searchVal:\n",
    "                out[k] = dictname[k]\n",
    "        if operator == '>':\n",
    "            if dictname[k] > searchVal:\n",
    "                out[k] = dictname[k]\n",
    "        if operator == '<':\n",
    "            if dictname[k] < searchVal:\n",
    "                out[k] = dictname[k]\n",
    "    return out\n",
    "\n",
    "\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# lst2 = listGen('a', 'b', 'c', 'd', 'e', 'f')\n",
    "# tbl = listsToDict(lst2, lst)\n",
    "# print('dict filtered:', dictFilter(tbl, 2, '>'))  # OK: {'c': 3, 'e': 'tst', 'd': 4, 'f': 55}\n",
    "\n",
    "\n",
    "def trnsps(datalist):  # fix ???????????????\n",
    "    nparr = np.empty([np.size(datalist)])\n",
    "    for i in range(np.size(datalist)):\n",
    "        nparr[i] = datalist[i]\n",
    "    return nparr\n",
    "\n",
    "\n",
    "# # fix\n",
    "# def listSlice(lstname, beg, end=0, endIncluded=True, posStep=1):\n",
    "#     \"\"\"slices one or more cols\n",
    "#     leave end=0 to slice one col only\"\"\"\n",
    "#     if posStep < 1:\n",
    "#         posStep = 1\n",
    "#     ln = len(lstname)\n",
    "#     begg = beg\n",
    "#     endd = end\n",
    "#     if end == 0:  # ignore end, slice only 1 col\n",
    "#         endd = beg\n",
    "#     if end < 0:  # convert neg idx to pos to facilitate below comp and cals\n",
    "#         endd = ln + end\n",
    "#     if beg < 0:  # convert neg idx to pos to facilitate below comp and cals\n",
    "#         begg = ln + beg\n",
    "#\n",
    "#     # if begg > endd:  # swap to avoid returning empty [] on reverse order error\n",
    "#     #     tmp = begg\n",
    "#     #     begg = endd\n",
    "#     #     endd = tmp\n",
    "#\n",
    "#     rev = False\n",
    "#     if begg > endd:  # swap to avoid returning empty [] on reverse order error\n",
    "#         rev = True\n",
    "#\n",
    "#     if endIncluded or endd == begg:\n",
    "#         if rev:\n",
    "#             begg += 1\n",
    "#         else:\n",
    "#             endd += 1\n",
    "#\n",
    "#     out = []\n",
    "#     if rev:\n",
    "#         out = lstname[endd:begg:posStep]\n",
    "#     else:\n",
    "#         out = lstname[begg:endd:posStep]\n",
    "#\n",
    "#     if rev:\n",
    "#         return listRev(out)\n",
    "#     else:\n",
    "#         return out\n",
    "\n",
    "# # EXAMPLE:::::::::::::::::::\n",
    "# lst = listGen(1, 2, 3, 4, 'tst', 55)\n",
    "# print(lst)\n",
    "# print(listItem(lst, -1))  # 55\n",
    "# # print(listItem(lst, 2:4))  # ERR\n",
    "# print(listSlice(lst, 1, 3, False))  # [2, 3]\n",
    "# print(listSlice(lst, 1, 3))  # [2, 3, 4]\n",
    "# print(listSlice(lst, 3, 1))  # [4, 3, 2]\n",
    "# print(listSlice(lst, 3, 0))  # [4]\n",
    "# print(listSlice(lst, 3))  # [4]\n",
    "# print(listSlice(lst, 3, 0, False))  # [4]\n",
    "# print(listSlice(lst, 3, endIncluded=False))  # [4]\n",
    "# print(listSlice(lst, 3, 3))  # [4]\n",
    "# print(listSlice(lst, 3, 3, False))  # [4]\n",
    "# print(listSlice(lst, 3, 4, False))  # [4]\n",
    "# print(listSlice(lst, 3, 4, True))  # [4, 'tst']\n",
    "# print(listSlice(lst, 1, -1))  # [2, 3, 4, 'tst', 55]\n",
    "# print(listSlice(lst, 0, -1))  # [1, 2, 3, 4, 'tst', 55]\n",
    "# print(listSlice(lst, 0, -1, False))  # [1, 2, 3, 4, 'tst']\n",
    "# print(listSlice(lst, -1, 0, False))  # ERR []...............................\n",
    "\n",
    "\n",
    "def listSlice(lstname, beg, end=None, rvrs=False):\n",
    "    begg = beg\n",
    "    endd = end\n",
    "    # if end == 0 or end == beg:\n",
    "    #     endd = beg + 1\n",
    "    out = lstname[begg:endd]\n",
    "    if rvrs:\n",
    "        return listRev(out)\n",
    "    else:\n",
    "        return out\n",
    "\n",
    "\n",
    "# # EXAMPLE:::::::::::::\n",
    "# lst = [0, 'one', 2, 3, 'four', 5, 6]\n",
    "# print(lst)  # [0, 'one', 2, 3, 'four', 5, 6]\n",
    "# print(listSlice(lst, 1, 1))  # []\n",
    "# print(listSlice(lst, 1))  # ['one', 2, 3, 'four', 5, 6]\n",
    "# print(listSlice(lst, -2))  # [5, 6]\n",
    "# print(listSlice(lst, 1, -1))  # ['one', 2, 3, 'four', 5]\n",
    "# print(lst[1:-1])  # ['one', 2, 3, 'four', 5]\n",
    "# print(listSlice(lst, 1, -1, True))  # [5, 'four', 3, 2, 'one']\n",
    "# print(listSlice(lst, -3, -1))  # ['four', 5]\n",
    "# print(listSlice(lst, -1, -1))  # []\n",
    "# print(listSlice(lst, -1, 0))  # []\n",
    "# print(listSlice(lst, -1, 2))  # []\n",
    "\n",
    "\n",
    "# https://realpython.com/python-kwargs-and-args/#ordering-arguments-in-a-function\n",
    "# https://stackoverflow.com/questions/40676085/why-cant-i-use-a-starred-expression\n",
    "# def listUnpackItems(listname):\n",
    "#     return *listname  # ERR\n",
    "\n",
    "# my_list = [1, 2, 3]\n",
    "# print(*my_list)  # ERR\n",
    "\n",
    "# my_list = [1, 2, 3, 4, 5, 6]\n",
    "# a, *b, c = my_list\n",
    "\n",
    "\n",
    "# def dictGen(**kvs):\n",
    "#     out = dict()\n",
    "#     for k, v in kvs:\n",
    "#         out[k] = v\n",
    "#     return out\n",
    "#\n",
    "# dictt = dictGen('st0': 92, 'st1': 67, 'st2': 78)\n",
    "# print('dictGen:::', dictt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfItem1(df, i, j):\n",
    "    return df.iat[i, j]\n",
    "\n",
    "\n",
    "def dfItem2(df, lblrow, lblcol):\n",
    "    return df.at[lblrow, lblcol]\n",
    "\n",
    "\n",
    "def dfItemSet1(df, i, j, val):\n",
    "    df.iat[i, j] = val\n",
    "\n",
    "\n",
    "def dfItemSet2(df, lblrow, lblcol, val):\n",
    "    df.at[lblrow, lblcol] = val\n",
    "\n",
    "\n",
    "def dfSlice1(df, ib=None, ie=None, jb=None, je=None):\n",
    "    return df.iloc[ib:ie, jb:je]\n",
    "\n",
    "\n",
    "def dfSlice2(df, ib=None, ie=None, listcols=[]):\n",
    "    out = pd.DataFrame()\n",
    "    if not listcols:\n",
    "        out = df.iloc[ib:ie, :]\n",
    "    else:\n",
    "        out = df.loc[ib:ie, listcols]\n",
    "    return out\n",
    "\n",
    "\n",
    "# # EXAMPLE:::::::::::::::::\n",
    "# df = pd.DataFrame(datasets.load_diabetes()['data'])\n",
    "# print_('\\nHEAD::::::::::::::::::::::::::\\n', df.head())\n",
    "# print_('\\nSLICE iLOC::::::::::::::::::::::::::\\n', dfSlice1(df, ie=3, jb=8))  # OK\n",
    "# df.columns = ['s0', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9']\n",
    "# print_('\\nSLICE LOC::::::::::::::::::::::::::\\n', dfSlice2(df, 1, 3, ['s0', 's3', 's4']))  # OK\n",
    "# print_('\\nSLICE LOC::::::::::::::::::::::::::\\n', dfSlice2(df, listcols=['s0', 's3', 's4']))  # OK\n",
    "# print_('\\nSLICE LOC::::::::::::::::::::::::::\\n', dfSlice2(df, 1, 3))  # OK\n",
    "\n",
    "\n",
    "def dfCatCol2Nums(dataframe, colname, forceWithNAN=True):\n",
    "    \"\"\"if a dataframe col has very few values repeating all over the col\n",
    "    use this function to automatically convert the values (no matter the data type)\n",
    "    to numbers from 0 to N based on their position in .unique() function result\n",
    "    forceWithNAN=True: will replace a value with NaN if not possible to convert to a number\n",
    "    forceWithNAN=False: will not apply this function at all if not possible to convert all the values\"\"\"\n",
    "    arrcats = dataframe[colname].unique()\n",
    "    print('')\n",
    "    print(str(colname) + ': Before convert to CAT numbers.......\\n')\n",
    "    print(dataframe[colname].unique())\n",
    "    print('')\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        rowcolval = row[colname]\n",
    "        #         icol = dataframe.columns.get_loc(colname)\n",
    "        for i, v in enumerate(arrcats):\n",
    "            if rowcolval == v:\n",
    "                dataframe.at[index, colname] = i\n",
    "                break\n",
    "                # https://stackoverflow.com/questions/15891038/change-data-type-of-columns-in-pandas\n",
    "    # 3 Methods: to_numeric(), astype(), infer_objects()\n",
    "    # df[\"a\"] = pd.to_numeric(df[\"a\"])\n",
    "    # df[[\"a\", \"b\"]] = df[[\"a\", \"b\"]].apply(pd.to_numeric)\n",
    "    if forceWithNAN:\n",
    "        dataframe[colname] = pd.to_numeric(dataframe[colname], downcast='integer',\n",
    "                                           errors='coerce')  # no floats coz its cat\n",
    "    else:\n",
    "        dataframe[colname] = pd.to_numeric(dataframe[colname], downcast='integer',\n",
    "                                           errors='ignore')  # no update at all if a single non-convertable found\n",
    "\n",
    "    print('')\n",
    "    print(str(colname) + ': After convert to CAT numbers.......\\n')\n",
    "    print(dataframe[colname].unique())\n",
    "    print('')\n",
    "    return dataframe.head()\n",
    "\n",
    "\n",
    "# EXAMPLE:::::::::::::::::::::::::\n",
    "# dfCatCol2Nums(df, 'city')\n",
    "\n",
    "# RESULT:::::::::::::::::::\n",
    "# city: Before convert to CAT numbers.......\n",
    "# ['melb' 'syd' 'perth' 'gold']\n",
    "# city: After convert to CAT numbers.......\n",
    "# [0 1 2 3]\n",
    "\n",
    "\n",
    "def dfStrCol2Date(dataframe, colname):\n",
    "    print('')\n",
    "    print(str(colname) + ': Before convert to DATE.......\\n')\n",
    "    print(dataframe[colname].unique())\n",
    "    print('')\n",
    "    dataframe[colname] = pd.to_datetime(dataframe[colname])\n",
    "    print('')\n",
    "    print(str(colname) + ': After convert to DATE.......\\n')\n",
    "    print(dataframe[colname].unique())\n",
    "    print('')\n",
    "    return dataframe.head()\n",
    "\n",
    "\n",
    "# EXAMPLE:::::::::::::::::::\n",
    "# dfStrCol2Date(df, 'sale_date')\n",
    "\n",
    "# RESULTS::::::::::::::::::\n",
    "# sale_date: Before convert to DATE.......\n",
    "# ['Wed May 21 00:00:00 EDT 2008' 'Tue May 20 00:00:00 EDT 2008'\n",
    "#  'Mon May 19 00:00:00 EDT 2008' 'Fri May 16 00:00:00 EDT 2008'\n",
    "#  'Thu May 15 00:00:00 EDT 2008']\n",
    "#\n",
    "# sale_date: After convert to DATE.......\n",
    "# ['2008-05-21T00:00:00.000000000' '2008-05-20T00:00:00.000000000'\n",
    "#  '2008-05-19T00:00:00.000000000' '2008-05-16T00:00:00.000000000'\n",
    "#  '2008-05-15T00:00:00.000000000']\n",
    "\n",
    "def dfIdxColNameUpdate(dataframe, newname):\n",
    "    dataframe.index.name = newname\n",
    "    return dataframe\n",
    "\n",
    "def dfColNameUpdate(dataframe, colname, newname):\n",
    "    try:\n",
    "        dataframe.rename(columns={colname: newname}, inplace=True, errors='raise')\n",
    "        print('')\n",
    "        print(str(colname) + ' column name is updated to: ' + str(newname))\n",
    "        print('')\n",
    "    except KeyError:\n",
    "        print('Column({}) does not exist!!!'.format(colname))\n",
    "    return dataframe.head()\n",
    "\n",
    "\n",
    "# EXAMPLE:::::::::::::::::::::\n",
    "# dfColNameUpdate(df, 'sale_date', 'May2008')\n",
    "\n",
    "# RESULT:::::::::::::::::::::::\n",
    "# sale_date column name is updated to: May2008\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dfmax(df, colnames=[]):\n",
    "    a=df\n",
    "    if colnames!=[]: a=df[colnames]\n",
    "    maxval=a.max(axis=0).max()\n",
    "    maxcol=a.max(axis=0).idxmax()\n",
    "    maxrow=a.max(axis=1).idxmax()\n",
    "    return maxval, maxcol, maxrow\n",
    "# val,colname,rowname=dfmax(feat_targ_df, ['5d_close_future_pct'])\n",
    "\n",
    "def dfcorrmax(df, targetcolname, skipcorr=False):\n",
    "    tmp=df\n",
    "    if not skipcorr: tmp=tmp.corr()\n",
    "    tmp=tmp[[targetcolname]] # select one col only\n",
    "    rowname=targetcolname\n",
    "    tmp = tmp.drop(labels=[rowname]) # drop index row by lable name\n",
    "    tmp=abs(tmp)\n",
    "    _,_,res=dfmax(tmp)\n",
    "    return res\n",
    "# maxcorrfeat=dfcorrmax(feat_targ_df, '5d_close_future_pct', False)\n",
    "# corr=feat_targ_df.corr()\n",
    "# maxcorrfeat=dfcorrmax(corr, '5d_close_future_pct', True)\n",
    "\n",
    "\n",
    "def str2Date(strdate):  # its using dataframe trick, improve with a simple way\n",
    "    # return datetime.strptime(strdate, '%b %d %Y %I:%M%p') # 'Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'\n",
    "    dfdate = pd.DataFrame([strdate], columns=['date'])  # str to df\n",
    "    dfdate['date'] = pd.to_datetime(dfdate['date'], errors='coerce')  # str to date\n",
    "    return dfdate['date'][0]\n",
    "\n",
    "\n",
    "# EXAMPLE:::::::::::::::::::::::::::\n",
    "# str2Date('Mon May 19 00:00:00 EDT 2008') # Timestamp('2008-05-19 00:00:00')\n",
    "\n",
    "\n",
    "def dateStr2Day(strdate):\n",
    "    return str2Date(strdate).day\n",
    "\n",
    "\n",
    "# EXAMPLE::::::::::::::::::::::::::::\n",
    "# dateStr2Day('Mon May 19 00:00:00 EDT 2008') # 19\n",
    "\n",
    "def date2Day(date):\n",
    "    #     df['year'] = df['ArrivalDate'].dt.year\n",
    "    return date.day\n",
    "\n",
    "\n",
    "# EXAMPLE:::::::::::::::::::::::::::::\n",
    "# date2Day(str2Date('Mon May 19 00:00:00 EDT 2008'))  # 19\n",
    "\n",
    "\n",
    "def dfDateCol2DayOfMonth(row, colname):\n",
    "    return row[colname].day\n",
    "\n",
    "\n",
    "# EXAMPLE1::::::::::::::::::::::::::::::\n",
    "# df['May2008']=df.apply(dfDateCol2DayOfMonth, colname='May2008', axis=1)\n",
    "\n",
    "# EXAMPLE2:::::::::::::::::::::::\n",
    "# df['May2008']=df['May2008'].apply(lambda dt: dt.day)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dict2df(dict):\n",
    "    data_dict = dict\n",
    "    data_items = data_dict. items()\n",
    "    data_list = list(data_items)\n",
    "    df = pd. DataFrame(data_list) \n",
    "    return df\n",
    "\n",
    "\n",
    "def df2nparr(dforseries, strtyp=None):\n",
    "    return dforseries.to_numpy(dtype=strtyp)\n",
    "\n",
    "def df2series(dataframe):\n",
    "    return dataframe.squeeze()\n",
    "\n",
    "import numpy as np\n",
    "def dfresize(df,idxlist=[''], NaNvalue=np.NaN):\n",
    "    # https://stackoverflow.com/questions/47600468/add-empty-row-with-index-in-a-pandas-dataframe?rq=1\n",
    "    if idxlist==['']:\n",
    "        return df\n",
    "    else:\n",
    "        df = df.reindex(df.index.values.tolist()+idxlist)\n",
    "        df = df.fillna(NaNvalue)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def listflat(lst):\n",
    "    flatten=list()\n",
    "    for l in lst:\n",
    "        flatten.append(l[0])\n",
    "    return flatten\n",
    "\n",
    "# try:\n",
    "#     predictions=listflat(predictions)\n",
    "# except Exception as err:\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "def nparr2dict(arr):\n",
    "    return dict(enumerate(arr.flatten(), 1))\n",
    "\n",
    "def nparr2df(arr,colnames=[], rownames=[], idxcolname=''):\n",
    "#     return pd.DataFrame(data=arr[1:,1:],    # values\n",
    "#               index=arr[1:,0],    # 1st column as index\n",
    "#               columns=arr[0,1:])  # 1st row as the column names\n",
    "    if idxcolname=='': idxcolname = 0\n",
    "    ranks=range(arr.shape[0])\n",
    "    headers=[1] # if 1D arr\n",
    "    try:\n",
    "        headers=range(arr.shape[1])\n",
    "    except Exception as err:\n",
    "        headers=[1] # if 1D arr\n",
    "    if len(headers)==len(colnames):\n",
    "        headers=colnames\n",
    "    if len(ranks)==len(rownames):\n",
    "        ranks=rownames\n",
    "    tmp = pd.DataFrame(data=arr, index=ranks, columns=headers) \n",
    "    tmp.index.name = idxcolname\n",
    "    return tmp \n",
    "\n",
    "def nparrshif(arr, cnt, alsoidxarr=False, direc='right'):\n",
    "    if direc=='right': \n",
    "        yy = np.empty(shape=cnt) # why fills up some vals instead of empty???\n",
    "        yy = np.append(yy,arr)\n",
    "        for i in range(0,cnt): \n",
    "            yy[i]=np.nan\n",
    "    else:\n",
    "        emp = np.empty(shape=cnt)\n",
    "        for i in range(0,cnt): \n",
    "            emp[i]=np.nan\n",
    "        yy = arr\n",
    "        yy = np.append(yy,emp)\n",
    "    \n",
    "    if alsoidxarr:\n",
    "        idx = list(range(0,len(arr)+cnt))\n",
    "        idx = np.array(idx)\n",
    "        return yy, idx\n",
    "    else:\n",
    "        return yy\n",
    "\n",
    "    \n",
    "def nparrresize(arr, newsize):\n",
    "    if newsize==len(arr): return arr\n",
    "    tmp = np.resize(arr, newsize) # shrink / expand(filled with repeated elements)\n",
    "    if newsize>len(arr):\n",
    "        for i in range(newsize-len(arr),0,-1): \n",
    "            tmp[-i]=np.nan\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getECDF(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points: n\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF: x\n",
    "    xx = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF: y\n",
    "    yy = np.arange(1, n + 1) / n\n",
    "    mx = np.max(yy)\n",
    "    sm = np.sum(yy)\n",
    "    if mx == sm:  # ERR: too big sample size led to mostly zeros\n",
    "        yy = np.arange(1, n + 1)\n",
    "    # arange: Return evenly spaced values within a given interval.\n",
    "    # numpy.arange([start, ]stop, [step, ]dtype=None)\n",
    "    # np.arange(3.0): array([0., 1., 2.]) #with stop\n",
    "    # np.arange(3,7): array([3, 4, 5, 6]) #with start\n",
    "    # np.arange(3,7,2): array([3, 5]) #with step\n",
    "\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "# sns.set()\n",
    "# sns.distplot(np.random.exponential(scale=5,size=1000), hist=False)\n",
    "# sns.distplot(np.random.binomial(5, 0.5, 1000), hist=False)\n",
    "# plt.show()\n",
    "\n",
    "def randomNumber(centre=0, std=1, digits=-1):\n",
    "    out = np.random.normal(centre, std)\n",
    "    if digits >= 0:\n",
    "        out = np.round(out, digits)\n",
    "    # print(type(out)) # <type 'numpy.float64'>\n",
    "    return out\n",
    "\n",
    "\n",
    "# # EXAMPLE::::::::::::::\n",
    "# print_(randomNumber())  # 0.814358647488\n",
    "# print_(randomNumber(10))  # 9.95363191927\n",
    "# print_(randomNumber(10, 1, 0))  # 1.0\n",
    "# print_(randomNumber(10, 1, 1))  # 11.2\n",
    "# print_(randomNumber(10, 1, 2))  # 8.75\n",
    "\n",
    "\n",
    "def randomNumbers(centre=0, std=1, size=100, seed=-1, digits=-1):\n",
    "    if seed >= 0:\n",
    "        np.random.seed(seed)\n",
    "    out = np.random.normal(centre, std, size)\n",
    "    if digits >= 0:\n",
    "        out = np.round(out, digits)\n",
    "    # print(type(out))  # <type 'numpy.ndarray'>\n",
    "    return out\n",
    "\n",
    "\n",
    "# # EXAMPLE::::::::::::::\n",
    "# print_(randomNumbers())  # [-1.51315041  0.34606895 ... 1.35842246]\n",
    "# print_(randomNumbers(10))  # [10.19409061  9.51818294 ... 9.13437134]\n",
    "# print_(randomNumbers(10, 1, 10, 0))  # [10. 11.  8.  9. 10. 11.  9. 12. 11.  9.]\n",
    "# print_(randomNumbers(10, 1, 10, 1))  # [ 9.8  9.7 10.7  9.9  9.5  9.8  7.7  9.7 11.7  9.7]\n",
    "# print_(randomNumbers(10, 1, 10, 2))  # [ 9.51 10.08 10.81  9.52  9.01 10.69  9.66 10.   11.91 10.32]\n",
    "\n",
    "\n",
    "def getRandomData(enmRandTyp, size=100, seed=-1, *args):\n",
    "    \"\"\"generates data\n",
    "    args: pass according to original python args like below\n",
    "    normal: meanLoc = args[0], stdScale = args[1]\n",
    "    exponential: meanScale = args[0]\n",
    "    poisson: trialsProbProduct = args[0]  --> nTrials * probability\n",
    "    binomial: nTrials = args[0], probability = args[1] \"\"\"\n",
    "\n",
    "    if seed >= 0:\n",
    "        np.random.seed(seed)\n",
    "    # non rare events: NORMAL for continues BINOMINAL for discrete\n",
    "    # very rare events: EXPONENTIAL for continues POSSION for discrete\n",
    "\n",
    "    if enmRandTyp == enmRandTyp_.normal:\n",
    "        meanLoc = args[0]\n",
    "        stdScale = args[1]\n",
    "        return np.random.normal(loc=meanLoc, scale=stdScale, size=size)\n",
    "    if enmRandTyp == enmRandTyp_.exponential:\n",
    "        meanScale = args[0]\n",
    "        return np.random.exponential(meanScale, size)\n",
    "    # Poisson distribution deals with number of occurences of an event in a time period\n",
    "    # whereas exponential distribution deals with the time between these events.\n",
    "    if enmRandTyp == enmRandTyp_.poisson:\n",
    "        trialsProbProduct = args[0]  # nTrials * probability\n",
    "        return np.random.poisson(trialsProbProduct, size)\n",
    "    # The Poisson distribution is the limit of the binomial distribution for large N.\n",
    "    if enmRandTyp == enmRandTyp_.binominal:\n",
    "        nTrials = args[0]\n",
    "        probability = args[1]\n",
    "        return np.random.binomial(nTrials, probability, size)\n",
    "\n",
    "    if enmRandTyp == enmRandTyp_.random:\n",
    "        return np.random.random(size)\n",
    "        # Return random floats in the half-open interval [0.0, 1.0)\n",
    "\n",
    "\n",
    "# # EXAMPLE:::::::::::::::::::\n",
    "# mean = 10\n",
    "# std = 2\n",
    "# data = getRandomData(enmRandTyp_.normal, 1000, 42, mean, std)\n",
    "# plotHisto(data, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_(data, cnt=3):\n",
    "    if cnt < 1:\n",
    "        cnt = 1\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return data.head(cnt)\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return data[:cnt]\n",
    "    else:  # set {} or list []\n",
    "        return list(data)[:cnt]\n",
    "\n",
    "\n",
    "def tail_(data, cnt=3):\n",
    "    if cnt < 1:\n",
    "        cnt = 1\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return data.tail(cnt)\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return data[-cnt:]\n",
    "    else:  # others like set {} or list []\n",
    "        return list(data)[-cnt:]\n",
    "\n",
    "\n",
    "def shape_(data):\n",
    "    try:\n",
    "        out = data.shape  # (1100,12)\n",
    "        # print(type(out))  # <type 'tuple'>\n",
    "        return out\n",
    "    except Exception as err:\n",
    "        try:\n",
    "            out = pd.DataFrame(data).shape\n",
    "            # print(type(out))  # <type 'tuple'>\n",
    "            return out\n",
    "        except Exception as err:\n",
    "            printError(err, shape_)\n",
    "\n",
    "\n",
    "def describe_(df, col='', prcntls=[.05, .25, .5, .75, .95]):\n",
    "    # df.describe(percentiles=[.05, .25, .5, .75, .95], include=\"all\")\n",
    "    try:\n",
    "        tmp = df\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            tmp = pd.DataFrame(df)\n",
    "        if col == '':\n",
    "            return pd.DataFrame(tmp.describe(percentiles=prcntls, include=\"all\"))\n",
    "        else:\n",
    "            return pd.DataFrame(tmp[col].describe(percentiles=prcntls, include=\"all\"))\n",
    "    except Exception as err:\n",
    "        printError(err, describe_)\n",
    "\n",
    "\n",
    "def shapeRows(data):\n",
    "    return shape_(data)[0]\n",
    "\n",
    "\n",
    "def shapeCols(data):\n",
    "    return shape_(data)[1]\n",
    "\n",
    "\n",
    "def one2zero(singlecolrow):\n",
    "    if singlecolrow == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return singlecolrow\n",
    "\n",
    "\n",
    "def applyFuncOnCol(dataframe, colname, func):\n",
    "    dataframe[colname] = dataframe[colname].apply(func)\n",
    "\n",
    "\n",
    "# EXAMPLE:::::::::::::::::::::::::\n",
    "# applyFuncOnCol(df, 'DupPerItem', one2zero)\n",
    "\n",
    "\n",
    "def unique_(data, warnings=False, warningThreshhold=99):\n",
    "    \"\"\"by default warningThreshhold=99  (percent)\"\"\"\n",
    "    try:\n",
    "        # return np.nunique(data)\n",
    "        tmp = data\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            tmp = pd.DataFrame(data)\n",
    "        rows = shapeRows(tmp)\n",
    "        res = tmp.nunique()  # <class 'pandas.core.series.Series'>\n",
    "        # convert unique Series table to a datafram table\n",
    "        res = pd.DataFrame(res, columns=['uniques'])  # <class 'pandas.core.frame.DataFrame'>\n",
    "        # add more columns to unique table\n",
    "        res['DupPerItem'] = np.round(rows / res['uniques'], 2)\n",
    "        applyFuncOnCol(res, 'DupPerItem', one2zero)\n",
    "        res['DupRatePerItem'] = np.round(res['DupPerItem'] / rows * 100, 2)\n",
    "        res['ColUnqRate'] = np.round(res['uniques'] / rows * 100, 2)\n",
    "        res['ColDupRate'] = np.round((rows - res['uniques']) / rows * 100, 2)\n",
    "        # print warnings if SINGLE val or FEW val columns detected\n",
    "        if warnings:\n",
    "            print_('\\n', prfxsufx('UNIQUE Values WARNINGS!!!'))\n",
    "            for row in res.itertuples():\n",
    "                # https://cmdlinetips.com/2018/12/how-to-loop-through-pandas-rows-or-how-to-iterate-over-pandas-rows/#:~:text=A%20better%20way%20to%20iterate,and%20return%20a%20named%20tuple.\n",
    "                rowlbl = row.Index  # SAME: rowlbl=row[0]\n",
    "                allwrong = False\n",
    "                if row.uniques == 1:  # SAME: row[1]\n",
    "                    allwrong = True\n",
    "                    print_(prfxsufx('WAR!!!-DROP'), 'Column(', rowlbl,\n",
    "                           ') is USELESS, due to containig only a SINGLE value.')\n",
    "                if not allwrong:\n",
    "                    if row.ColDupRate >= warningThreshhold:  # SAME: row[5]\n",
    "                        print_(prfxsufx('IMP!-DROP/ENCODE(cat/ordinal)'), 'Column(', rowlbl,\n",
    "                               ') can be USELESS, due to containig very FEW values highly DUPLICATED.')\n",
    "                    elif row.uniques > 0:\n",
    "                        print_(prfxsufx('INFO-ENCODE(cat/ordinal)'), 'Column(', rowlbl, ') has some DUPLICATE values.')\n",
    "                allwrong = False\n",
    "        return res\n",
    "    except Exception as err:\n",
    "        printError(err, unique_)\n",
    "\n",
    "\n",
    "def dtypes_(data):\n",
    "    try:\n",
    "        tmp = data\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            tmp = pd.DataFrame(data)\n",
    "        return tmp.dtypes\n",
    "    except Exception as err:\n",
    "        printError(err, dtypes_)\n",
    "\n",
    "\n",
    "def isna_(data, warnings=False, warningThreshhold=50):\n",
    "    \"\"\"warningThreshhold=50 by default in percent\"\"\"\n",
    "    try:\n",
    "        tmp = data\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            tmp = pd.DataFrame(data)\n",
    "        rows = shapeRows(tmp)\n",
    "        res = tmp.isna().sum()  # <class 'pandas.core.series.Series'>\n",
    "        # convert unique Series table to a datafram table\n",
    "        res = pd.DataFrame(res, columns=['nulls'])  # <class 'pandas.core.frame.DataFrame'>\n",
    "        # add more columns to unique table\n",
    "        res['ColNullRate'] = np.round(res['nulls'] / rows * 100, 2)\n",
    "        res['ColFillRate'] = np.round((rows - res['nulls']) / rows * 100, 2)\n",
    "        # print warnings if NULL columns detected\n",
    "        if warnings:\n",
    "            print_('\\n', prfxsufx('NULL Values WARNINGS!!!'))\n",
    "            for row in res.itertuples():\n",
    "                # https://cmdlinetips.com/2018/12/how-to-loop-through-pandas-rows-or-how-to-iterate-over-pandas-rows/#:~:text=A%20better%20way%20to%20iterate,and%20return%20a%20named%20tuple.\n",
    "                rowlbl = row.Index  # SAME: rowlbl=row[0]\n",
    "                allwrong = False\n",
    "                if row.nulls == rows:\n",
    "                    allwrong = True\n",
    "                    print_(prfxsufx('WAR!!!-DROP'), 'Column(', rowlbl, ') is fully NULL and USELESS.')\n",
    "                if not allwrong:\n",
    "                    if row.ColNullRate >= warningThreshhold:\n",
    "                        print_(prfxsufx('IMP!-DROP/FillNA(mean/mode/median/prev)'), 'Column(', rowlbl,\n",
    "                               ') can be USELESS, due to highly missing values.')\n",
    "                    elif row.nulls > 0:\n",
    "                        print_(prfxsufx('INFO-FillNA(mean/mode/median/prev)'), 'Column(', rowlbl,\n",
    "                               ') has some MISSING values need to be FILLED.')\n",
    "                allwrong = False\n",
    "        return res\n",
    "    except Exception as err:\n",
    "        printError(err, isna_)\n",
    "\n",
    "\n",
    "# # # cnfg = sv.FeatureConfig(skip=\"PassengerId\", force_cat=[\"Ticket\"], force_text=[''],force_num=[''])\n",
    "# # # my_report = sv.compare_intra(train, train[\"Sex\"] == 'male', [\"Male\", \"Female\"], 'targetcol', cnfg)\n",
    "# # def getEDASweetvisCats(data, srcColName, operator='==', compareVal='', catNames=[str(compareVal),'cat2'], outputHTML='', targetCol=''):\n",
    "# #     svreport=''\n",
    "# #     if outputHTML=='':\n",
    "# #         outputHTML='svReport.html'\n",
    "# #     svreport = sv.compare_intra(data, compare=data2, target_feat=targetCol)\n",
    "# #     if svreport!='':\n",
    "# # #         print(type(svreport)) # <class 'sweetviz.dataframe_report.DataframeReport'>\n",
    "# #         svreport.show_html(outputHTML)\n",
    "# #         return showHTML(outputHTML)\n",
    "\n",
    "# def getEDASweetvis(data, outputHTML='', targetCol='', data2=None):\n",
    "#     svreport = ''\n",
    "#     if outputHTML == '':\n",
    "#         outputHTML = 'svReport.html'\n",
    "#     if data2 is None:\n",
    "#         svreport = sv.analyze(data, target_feat=targetCol)\n",
    "#     else:\n",
    "#         svreport = sv.compare(data, data2, target_feat=targetCol)\n",
    "#     if svreport != '':\n",
    "#         # print(type(svreport)) # <class 'sweetviz.dataframe_report.DataframeReport'>\n",
    "#         if targetCol != '':\n",
    "#             print_('\\n::::::::: Column ( ', targetCol, ' ) CORRELATIONS:::::::::\\n',\n",
    "#                    svreport.get_what_influences_me(targetCol))\n",
    "#         # print_('dataframe_summary_html::::::\\n', svreport.dataframe_summary_html) # <div class=\"container-feature-summar...\n",
    "#         svreport.show_html(outputHTML)\n",
    "#         return showHTML(outputHTML)\n",
    "\n",
    "\n",
    "def getEDA(data, dupThreshhold=99, nullThreshhold=50):\n",
    "    \"\"\"print and plot EDA like max min mean ecdf ... boxplot wiskerplot\"\"\"\n",
    "    # https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15\n",
    "    # https://www.kaggle.com/regivm/data-cleaning-and-eda-tutorial\n",
    "    # https://towardsdatascience.com/powerful-eda-exploratory-data-analysis-in-just-two-lines-of-code-using-sweetviz-6c943d32f34\n",
    "    # https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "    try:\n",
    "        print_('\\n', prfxsufx('TYPE'), '\\n', type(data))  # <class 'pandas.core.frame.DataFrame'>\n",
    "        print_('\\n', prfxsufx('SHAPE'), '\\n', shape_(data))  # <type 'tuple'>\n",
    "        print_('\\n', prfxsufx('HEAD'), '\\n', head_(data))  # <class 'pandas.core.frame.DataFrame'>\n",
    "        print_('\\n', prfxsufx('TAIL'), '\\n', tail_(data))  # <class 'pandas.core.frame.DataFrame'>\n",
    "        print_('\\n', prfxsufx('DTYPES'), '\\n', dtypes_(data))  # <class 'pandas.core.series.Series'>\n",
    "        print_('\\n', prfxsufx('NULLs'), '\\n', isna_(data, True, nullThreshhold))  # <class 'pandas.core.series.Series'>\n",
    "        print_('\\n', prfxsufx('UNIQUE Values'), '\\n',\n",
    "               unique_(data, True, dupThreshhold))  # <class 'pandas.core.frame.DataFrame'>\n",
    "        # print exact dup rows\n",
    "        # plot chart\n",
    "        # plot pairplots\n",
    "        # corr heatmap\n",
    "        print_('\\n', prfxsufx('CORRELATIONS'), '\\n', data.corr())\n",
    "        print_('\\n', prfxsufx('STATS'), '\\n', describe_(data))\n",
    "        # getEDASweetvis(df,'svGold.html','close')\n",
    "    except Exception as err:\n",
    "        printError(err, getEDA)\n",
    "\n",
    "\n",
    "def dropColsByUniqueValsCount(data, uniqueCount=1, includeLessEquals=True):\n",
    "    print('')\n",
    "    print('::::::::::: Dropping {}-Val columns is requested:::::::'.format(uniqueCount))\n",
    "    try:\n",
    "        df = data\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            df = pd.DataFrame(data)\n",
    "        # get number of unique values for each column\n",
    "        counts = df.nunique()\n",
    "        # record columns to delete\n",
    "        to_del = []\n",
    "        if includeLessEquals:\n",
    "            to_del = [i for i, v in enumerate(counts) if v <= uniqueCount]\n",
    "        else:\n",
    "            to_del = [i for i, v in enumerate(counts) if v == uniqueCount]\n",
    "        if shapeRows(to_del) > 0:\n",
    "            print_('\\n', prfxsufx('UNIQUE Values'), '\\n', unique_(data))\n",
    "            print_('\\n', prfxsufx('{}-VAL cols to drop'.format(uniqueCount)), '\\n', to_del, '\\n', df.columns[to_del])\n",
    "            print_('\\n', prfxsufx('HEAD before drop'), '\\n', df.head())\n",
    "            print_('\\n', prfxsufx('SHAPE before drop'), df.shape)\n",
    "            # drop useless columns\n",
    "            df.drop(df.columns[to_del], axis=1, inplace=True)\n",
    "            print_('\\n', prfxsufx('SHAPE after drop'), df.shape)\n",
    "            print_('\\n', prfxsufx('HEAD after drop'), '\\n', df.head())\n",
    "            print('')\n",
    "            return shapeRows(to_del)  # rets num of dropped cols\n",
    "        else:\n",
    "            print('\\n', prfx('no {}-VAL cols found to drop.'.format(uniqueCount)))\n",
    "            return 0\n",
    "    except Exception as err:\n",
    "        printError(err, dropColsByUniqueValsCount)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def dropColsWithSingleVal(data):\n",
    "    return dropColsByUniqueValsCount(data, 1, False)\n",
    "\n",
    "\n",
    "def dropColsWithFewVals(data, uniquesToRowsPercent=1):\n",
    "    opst = 100 - uniquesToRowsPercent\n",
    "    print('')\n",
    "    print(\n",
    "        '::::::::::: Dropping FEW-Val columns is requested (less than {}% Col uniqueness = more that {}% Col duplicated vals):::::::'.format(\n",
    "            uniquesToRowsPercent, opst))\n",
    "    try:\n",
    "        df = data\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            df = pd.DataFrame(data)\n",
    "        # get number of unique values for each column\n",
    "        counts = df.nunique()\n",
    "        # record columns to delete\n",
    "        to_del = [i for i, v in enumerate(counts)\n",
    "                  if (float(v) / df.shape[0] * 100) <= uniquesToRowsPercent]\n",
    "        if shapeRows(to_del) > 0:\n",
    "            print_('\\n', prfxsufx('UNIQUE Values'), '\\n', unique_(data))\n",
    "            print_('\\n', prfxsufx('FEW-Val cols to drop'), '\\n', 'Col Quantity:::: ', to_del, '\\n', 'Col List:::::',\n",
    "                   '\\n', df.columns[to_del])\n",
    "            print_('\\n', prfxsufx('HEAD before drop'), '\\n', df.head())\n",
    "            print_('\\n', prfxsufx('SHAPE before drop'), df.shape)\n",
    "            # drop useless columns\n",
    "            df.drop(df.columns[to_del], axis=1, inplace=True)\n",
    "            print_('\\n', prfxsufx('SHAPE after drop'), df.shape)\n",
    "            print_('\\n', prfxsufx('HEAD after drop'), '\\n', df.head())\n",
    "            print('')\n",
    "            return shapeRows(to_del)  # rets num of dropped cols\n",
    "        else:\n",
    "            print('\\n', prfx('no FEW-Val cols found to drop.'))\n",
    "            return 0\n",
    "    except Exception as err:\n",
    "        printError(err, dropColsWithFewVals)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLine(XArr, YArr, title='TITLE', xlabel='XLABEL', ylabel='YLABEL', *moreXYArrs):\n",
    "    \"\"\"draws a dotted line chart\"\"\"\n",
    "    \"\"\"moreXYArrs: pass each x and y arrays as nested list like [arrx1,arry1],[arrx2,arry2],...\"\"\"\n",
    "#     sns.set()\n",
    "    cntr = 1\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.margins(0.02)\n",
    "    plt.plot(XArr, YArr, marker='.', linestyle='none')\n",
    "    if np.array(moreXYArrs).size > 0:\n",
    "        # for i in range(len(moreXYArrs)):\n",
    "        #     x = moreXYArrs[i][0]\n",
    "        #     y = moreXYArrs[i][1]\n",
    "        #     plt.plot(x, y, marker='.', linestyle='none')\n",
    "        #     cntr += 1\n",
    "        for xy in moreXYArrs:\n",
    "            x = xy[0]\n",
    "            y = xy[1]\n",
    "            plt.plot(x, y, marker='.', linestyle='none')\n",
    "            cntr += 1\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    print(cntr, 'lines plotted')\n",
    "\n",
    "\n",
    "# def plotLine2D(XYTable, title='TITLE', xlabel='XLABEL', ylabel='YLABEL', *moreXYTables):\n",
    "def plotLine2D(XYTable, title='TITLE', xlabel='XLABEL', ylabel='YLABEL'):\n",
    "    \"\"\"...\"\"\"\n",
    "    xArr = (np.array(XYTable)[:, 0])  # [1.3006865 , 0.95819356,\n",
    "    # xArr = (np.array(XYArr)[:, 0:1])  # [[1.3006865 ],[0.95819356]\n",
    "    yArr = (np.array(XYTable)[:, 1])\n",
    "\n",
    "    # moreArrs = np.empty(len(moreXYTables))\n",
    "    # # moreArrs = np.array([])\n",
    "    # if np.array(moreXYTables).size > 0:\n",
    "    #     for i in range(len(moreXYTables)):\n",
    "    #         xarr = np.array(moreXYTables[i])[:, 0]\n",
    "    #         yarr = np.array(moreXYTables[i])[:, 1]\n",
    "    #         moreArrs[i] = [xarr, yarr]\n",
    "    #     # for xyTbl in moreXYTables:\n",
    "    #     #     xarr = np.array(xyTbl)[:, 0]\n",
    "    #     #     yarr = np.array(xyTbl)[:, 1]\n",
    "    #     #     # xyarr=np.array([])\n",
    "    #     #     # xyarr=np.append(xyarr,xarr)\n",
    "    #     #     # xyarr = np.append(xyarr, yarr)\n",
    "    #     #     # np.append(moreArrs, xyarr)\n",
    "    #     #     np.append(moreArrs, [xarr, yarr])\n",
    "\n",
    "    # plotLine(xArr, yArr, title, xlabel, ylabel, moreArrs)\n",
    "    plotLine(xArr, yArr, title, xlabel, ylabel)\n",
    "\n",
    "\n",
    "def plotHisto(XArr, title='TITLE', xlabel='XLABEL', ylabel='YLABEL', bins=10, *moreXArrs):\n",
    "    \"\"\"draws a histogram chart\"\"\"\n",
    "#     sns.set()\n",
    "    cntr = 1\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.margins(0.02)\n",
    "    # for leg in legends:\n",
    "    # plt.legend(leg)\n",
    "    plt.hist(XArr, bins=bins, density=True, histtype='step')  # outdated: normed=True use density\n",
    "    if np.array(moreXArrs).size > 0:\n",
    "        for arr in moreXArrs:\n",
    "            plt.hist(arr, bins=bins, density=True, histtype='step')  # outdated: normed=True use density\n",
    "            cntr += 1\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    print(cntr, 'histograms plotted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class enmCorr_(Enum):\n",
    "    pearson, kendall, spearman = range(3)\n",
    "\n",
    "def corr_(df, enmCorr=enmCorr_.pearson):\n",
    "    # 'pearson', 'kendall', 'spearman'\n",
    "    cor = df.corr(method='pearson')\n",
    "    if enmCorr == enmCorr_.kendall:\n",
    "        cor = df.corr(method='kendall')\n",
    "    if enmCorr == enmCorr_.spearman:\n",
    "        cor = df.corr(method='spearman')\n",
    "    return cor\n",
    "\n",
    "def plotHeatmap(df, ttl='Correlation', enmCorr=enmCorr_.pearson, nrows=14, ncols=12, maskenabled=True, dfHasCorr=False):\n",
    "    cor = df\n",
    "    if not dfHasCorr: cor = corr_(df, enmCorr)\n",
    "    import numpy as np\n",
    "    if maskenabled:\n",
    "        mask = np.zeros_like(cor)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    import seaborn as sns\n",
    "    _, ax = plt.subplots(figsize=(nrows, ncols))\n",
    "    colormap = sns.diverging_palette(220, 10, s=100, l=20, as_cmap=True)\n",
    "    if maskenabled:\n",
    "        _ = sns.heatmap(\n",
    "            cor,\n",
    "            cmap=colormap,\n",
    "            square=True,\n",
    "            cbar_kws={'shrink': .9},\n",
    "            ax=ax,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            linewidths=0.1, vmax=1.0, linecolor='white',\n",
    "            annot_kws={'fontsize': 12}\n",
    "        )\n",
    "    else:\n",
    "        _ = sns.heatmap(\n",
    "            cor,\n",
    "            cmap=colormap,\n",
    "            square=True,\n",
    "            cbar_kws={'shrink': .9},\n",
    "            ax=ax,\n",
    "            annot=True,\n",
    "            linewidths=0.1, vmax=1.0, linecolor='white',\n",
    "            annot_kws={'fontsize': 12}\n",
    "        )\n",
    "    plt.title(ttl, y=1.05, size=15)\n",
    "# # EXAMPLE::::::::::::::::::::::::\n",
    "# plotHeattmap(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pltprepaxarrs(y_actwithdates, y_pred, printlog=False):\n",
    "    import numpy as np\n",
    "    series = y_actwithdates\n",
    "    y = series.values\n",
    "    predictions = np.array(y_pred)\n",
    "    \n",
    "    yyyact = y[-len(predictions):]\n",
    "    yyypred = predictions\n",
    "    xxx = series.index[-len(yyypred):]\n",
    "    \n",
    "    if printlog==True:\n",
    "        print('Act=',yyyact[0],yyyact[-1],len(yyyact), type(yyyact))\n",
    "        print('Pred=',yyypred[0],yyypred[-1],len(yyypred), type(yyypred))\n",
    "        print('Idx=',xxx[0],xxx[-1],len(xxx), type(xxx))\n",
    "    return xxx, yyyact, yyypred\n",
    "# xxx,yyyact,yyypred=pltprepaxarrs(series, predictions, True)\n",
    "\n",
    "\n",
    "def pltpyplot(xarr, yarrlist, ttl='', xlbl='', ylbl='', zoomrows=0,  \n",
    "              legendlist=['','','','','','','','',''], xyrotate=[270,0], \n",
    "              markerlist=['o','o','.','.','x','s','^','d','*'], pngfile=''):\n",
    "    from matplotlib import pyplot as plt\n",
    "    xxx = xarr\n",
    "    try: xxx = xxx.flatten() \n",
    "    except: pass\n",
    "    if zoomrows>0: xxx = xxx[-zoomrows:]\n",
    "    for i in range(len(yarrlist)):\n",
    "        yyyact = yarrlist[i]\n",
    "        try: yyyact = yyyact.flatten() \n",
    "        except: pass\n",
    "        if zoomrows>0: yyyact = yyyact[-zoomrows:]\n",
    "        plt.plot(xxx, yyyact, label=legendlist[i], marker=markerlist[i])\n",
    "    plt.title(ttl)\n",
    "    plt.xlabel(xlbl)\n",
    "    plt.ylabel(ylbl)\n",
    "    plt.xticks(rotation=xyrotate[0])\n",
    "    plt.yticks(rotation=xyrotate[1])\n",
    "    plt.legend()\n",
    "    if pngfile!='': plt.savefig(pngfile)\n",
    "    plt.show()\n",
    "# xxx,yyyact,yyypred=pltprepaxarrs(series, y_pred, True)\n",
    "# pltplotly(xxx, [yyyact,yyypred], ttl='title', xlbl='xlbl', ylbl='ylbl', legendlist=['Act','Pred'], htmlfile='ex.htm')\n",
    "\n",
    "\n",
    "def pltplotly(xarr, yarrlist, ttl='', xlbl='', ylbl='', zoomrows=0, \n",
    "              legendlist=['','','','','','','','',''], htmlfile=''):\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    xxx = xarr\n",
    "    try: xxx = xxx.flatten() \n",
    "    except: pass\n",
    "    if zoomrows>0: xxx = xxx[-zoomrows:]\n",
    "    fig = go.Figure()\n",
    "    for i in range(len(yarrlist)):\n",
    "        yyyact = yarrlist[i]\n",
    "        try: yyyact = yyyact.flatten() \n",
    "        except: pass\n",
    "        if zoomrows>0: yyyact = yyyact[-zoomrows:]\n",
    "        fig.add_trace(go.Scattergl(x=xxx, y=yyyact, mode='lines+markers', name=legendlist[i]))\n",
    "    fig.update_layout(title=ttl, xaxis_title=xlbl, yaxis_title=ylbl)\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    if htmlfile!='': fig.write_html(htmlfile)\n",
    "    fig.show()\n",
    "# xxx,yyyact,yyypred=pltprepaxarrs(series, y_pred, True)\n",
    "# pltplotly(xxx, [yyyact,yyypred], ttl='title', xlbl='xlbl', ylbl='ylbl', legendlist=['Act','Pred'], htmlfile='ex.htm')\n",
    "\n",
    "\n",
    "def pltpyplothist(data, ttl='', xlbl='', zoomrows=0, \n",
    "                  numbins=25, inchsize=(10,10), xyrotate=[0,0], pngfile=''):\n",
    "    from matplotlib import pyplot as plt\n",
    "    yyyact = data\n",
    "    if zoomrows>0: yyyact = yyyact[-zoomrows:]\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    yyyact.hist(bins=numbins, figsize=inchsize, xrot=xyrotate[0], yrot=xyrotate[1])\n",
    "    plt.title(ttl)\n",
    "    plt.xlabel(xlbl)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=xyrotate[0])\n",
    "    plt.yticks(rotation=xyrotate[1])\n",
    "    plt.subplot(212)\n",
    "    yyyact.plot(kind='kde')\n",
    "    plt.xlabel(xlbl)\n",
    "    plt.xticks(rotation=xyrotate[0])\n",
    "    plt.yticks(rotation=xyrotate[1])\n",
    "    if pngfile!='': plt.savefig(pngfile)\n",
    "    plt.show()\n",
    "# pltpyplothist(series, ttl='Retail Distribution', xlbl='Sales ($b)', pngfile='pltdatadist.png')\n",
    "\n",
    "\n",
    "def pltplotlyhist(data, ttl='', xlbl='', zoomrows=0, binwidth=1, htmlfile=''):\n",
    "    import numpy as np\n",
    "    import plotly.figure_factory as ff\n",
    "    import plotly.graph_objects as go\n",
    "    fig = go.Figure()\n",
    "    yyyact = np.array(data)\n",
    "    if zoomrows>0: yyyact = yyyact[-zoomrows:]\n",
    "    yyyact = [yyyact]\n",
    "    group_labels = [''] # name of the dataset\n",
    "    fig = ff.create_distplot(yyyact, group_labels, bin_size=binwidth) # , bin_size=700 n 25 n \n",
    "    fig.update_layout(title=ttl, xaxis_title=xlbl, yaxis_title='Density')\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    if htmlfile!='': fig.write_html(htmlfile)\n",
    "    fig.show()\n",
    "# pltplotlyhist(series, ttl='Retail Distribution', xlbl='Sales ($b)', binwidth=.5, htmlfile='pltdatadist.htm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split2traintest(data, splitrateL=0.9, keepasdataframe=False):\n",
    "    series=data\n",
    "    # prepare data\n",
    "    X = series.values\n",
    "    X = X.astype('float32')\n",
    "    train_size = int(len(X) * splitrateL)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    if keepasdataframe:\n",
    "        train = pd.DataFrame(train)\n",
    "        train.index = series.index[:len(train)]\n",
    "        test = pd.DataFrame(test)\n",
    "        test.index = series.index[-len(test):]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationByDecompose(data, maxlag=18, hastrend=True, lastrowsnum=0, showchart=True, filetitle=''):\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # N = len(series)\n",
    "    N = maxlag\n",
    "#     original = read_csv('original.csv', header=0, index_col=0)\n",
    "    original = data\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig2 = go.Figure()\n",
    "\n",
    "    lstseasonval=[]\n",
    "    lstseasonvals=[]\n",
    "    for i in range(1,N+1):\n",
    "        try:\n",
    "#             series = original[-100:]\n",
    "            series = original\n",
    "            if lastrowsnum >= 2*maxlag > 0: series = series[-lastrowsnum:]\n",
    "            mod = 'additive'\n",
    "            if hastrend: mod='multiplicative'\n",
    "            result = seasonal_decompose(series, model=mod, period=i)\n",
    "            series = result.seasonal\n",
    "            lstseasonval.append(max(series))\n",
    "            lstseasonvals.append(series)\n",
    "\n",
    "            if showchart: \n",
    "                fig.add_trace( go.Scattergl(x=series.index.tolist(), y=series.values.tolist(), \n",
    "                                      mode='lines+markers', name='period({})'.format(i)) )\n",
    "        except Exception as err:\n",
    "            print('ERROR for period({}): {}'.format(i, err))\n",
    "            continue\n",
    "    bestlag = lstseasonval.index(max(lstseasonval))+1\n",
    "    station = lstseasonvals[bestlag-1]\n",
    "    fil = 'decompseason'\n",
    "    if filetitle != '': fil += filetitle\n",
    "    fil += '.csv'\n",
    "    station.to_csv(fil) # header=False\n",
    "\n",
    "    \n",
    "    if showchart:\n",
    "        fig.update_layout( title='Decomposition', xaxis_title='Time', yaxis_title='Seasonal')\n",
    "        fig.update_xaxes(rangeslider_visible=True)\n",
    "        fig.write_html(\"pltdecompseasonAll\"+filetitle+\".html\")\n",
    "        fig.show()\n",
    "\n",
    "        fig2.add_trace( go.Scattergl(y=lstseasonval, mode='lines+markers') )\n",
    "        fig2.update_layout( title='max season val (Highest is for optimum lag)', xaxis_title='period lag', yaxis_title='season val')\n",
    "        fig2.update_xaxes(rangeslider_visible=True)\n",
    "        fig.write_html(\"pltdecompseasonOpt\"+filetitle+\".html\")\n",
    "        fig2.show()\n",
    "        \n",
    "        \n",
    "        # OPTIMISED DECOMPOSE PLOT\n",
    "        series = original\n",
    "        if lastrowsnum >= 2*maxlag > 0: series = series[-lastrowsnum:]\n",
    "        result = seasonal_decompose(series, model=mod, period=bestlag) # model='additive' or model='multiplicative'\n",
    "\n",
    "        series = result.observed\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace( go.Scattergl(x=series.index.tolist(), y=series.values.tolist(), \n",
    "                                  mode='lines+markers', name='Target') )\n",
    "        fig.update_layout( title='Decomposition', xaxis_title='Time', yaxis_title='Observed')\n",
    "        fig.update_xaxes(rangeslider_visible=True)\n",
    "        fig.write_html(\"pltdecompObserved\"+filetitle+\".html\")\n",
    "        fig.show()\n",
    "\n",
    "        series = result.trend\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace( go.Scattergl(x=series.index.tolist(), y=series.values.tolist(), \n",
    "                                  mode='lines+markers', name='Target') )\n",
    "        fig.update_layout( title='Decomposition', xaxis_title='Time', yaxis_title='Trend')\n",
    "        fig.update_xaxes(rangeslider_visible=True)\n",
    "        fig.write_html(\"pltdecompTrend\"+filetitle+\".html\")\n",
    "        fig.show()\n",
    "\n",
    "        series = result.seasonal\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace( go.Scattergl(x=series.index.tolist(), y=series.values.tolist(), \n",
    "                              mode='lines+markers', name='Target') )\n",
    "        fig.update_layout( title='Decomposition', xaxis_title='Time', yaxis_title='Seasonal')\n",
    "        fig.update_xaxes(rangeslider_visible=True)\n",
    "        fig.write_html(\"pltdecompSeasonal\"+filetitle+\".html\")\n",
    "        fig.show()\n",
    "\n",
    "        series = result.resid\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace( go.Scattergl(x=series.index.tolist(), y=series.values.tolist(), \n",
    "                                  mode='lines+markers', name='Target') )\n",
    "        fig.update_layout( title='Decomposition', xaxis_title='Time', yaxis_title='Residual')\n",
    "        fig.update_xaxes(rangeslider_visible=True)\n",
    "        fig.write_html(\"pltdecompResidual\"+filetitle+\".html\")\n",
    "        fig.show()\n",
    "        \n",
    "    print('Best Seasonal Lag:::::', bestlag)\n",
    "    return bestlag, station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talib_SMA(arr, timeperiod=2): # https://stackoverflow.com/questions/50482884/module-pandas-has-no-attribute-rolling-mean\n",
    "    import pandas as pd\n",
    "    return(pd.Series(arr).rolling(timeperiod).mean().values)\n",
    "\n",
    "def talib_RSI(arr, timeperiod=2): # https://stackoverflow.com/questions/20526414/relative-strength-index-in-python-pandas\n",
    "    n=timeperiod\n",
    "    import pandas as pd\n",
    "    price = pd.DataFrame(arr,columns=['Close'])\n",
    "    delta = price['Close'].diff()\n",
    "    dUp, dDown = delta.copy(), delta.copy()\n",
    "    dUp[dUp < 0] = 0\n",
    "    dDown[dDown > 0] = 0\n",
    "    dUp=dUp.fillna(0)\n",
    "    dDown=dDown.fillna(0)\n",
    "    RolUp = dUp.rolling(n).mean()\n",
    "    RolDown = dDown.rolling(n).mean().abs()\n",
    "    RS = RolUp / RolDown\n",
    "    rsi= 100.0 - (100.0 / (1.0 + RS))\n",
    "    return rsi.values\n",
    "\n",
    "\n",
    "\n",
    "# data=[5,3,8,2,6,1,8,9,4,5]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(data)\n",
    "# plt.plot(talib_SMA(data, timeperiod=2))\n",
    "# plt.plot(talib_SMA(data, timeperiod=5))\n",
    "# plt.plot(talib_SMA(data, timeperiod=9))\n",
    "# plt.title('MA indicators')\n",
    "# plt.show()\n",
    "\n",
    "# data=[5,3,8,2,6,1,8,9,4,5]\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot([])\n",
    "# plt.plot(talib_RSI(data, timeperiod=2))\n",
    "# plt.plot(talib_RSI(data, timeperiod=5))\n",
    "# plt.plot(talib_RSI(data, timeperiod=9))\n",
    "# plt.title('RSI indicators')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell exec done\n"
     ]
    }
   ],
   "source": [
    "print('cell exec done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
